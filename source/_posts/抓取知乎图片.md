---
title: 抓取知乎图片
date: 2018-03-31 22:46:08
categories: 脚本
copyright: true
tags: 
    - 爬虫
    - 脚本
description:
---
## 动机
今天突然想找一点好看的壁纸，便在网上搜索“程序员都在使用什么壁纸”，好巧不巧地点进知乎一个提问，发现其中的答主提供的一些壁纸都挺符合口味的，但一看那么多如果要一张一张下太过麻烦，便决定动手写一个爬取该页图片的脚本。
## 过程
首先打开网页调试器找到图片的元素，可以发现知乎正文的图片放在一个叫`noscript`的标签中，这里用BeautifulSoup提供的功能提取出链接，注意这里应该使用`data-original`而不是`src`，因为`src`显示的图片是压缩过的，而`data-original`是改图片对应的源文件:
```Python
noscripts = soup.findAll('noscript')
imgurls = []
for noscript in noscripts:
    img = noscript.find('img')
    url = img['data-original'] if img['data-original'] else ''
    if url != '':
        imgurls.append(url)
```

* 获取图片地址
![获取图片地址](抓取知乎图片/获取图片地址.png)
然后创建保存图片的路径，这里是针对Posix系统的路径处理，判断是否是从根路径开始的，如果不是则在当前路径创建，最后在路径上拼接`/`用来连接文件名：
```Python
if save_path[0] != '/':
    save_path = './' + save_path
if not os.path.exists(save_path):
    os.makedirs(save_path)

if save_path[-1] != '/':
    save_path += '/'
```

最后便是通过`urlretrieve`方法下载图片并保存到相应的文件，文件名是使用`uuid`生成的防止重复：
```Python
for imgUrl in imgurls:
    imgnum = imgnum + 1
    picname = save_path + str(uuid.uuid1())+'.jpg'
    print('正在下载%d张' % imgnum)
    try:
        req.urlretrieve(imgUrl, picname)
    except urllib.error.URLError as e:
        if hasattr(e, 'code') or hasattr(e, 'reason'):
            print('第%d张下载失败，其链接地址为%s' % (imgnum, imgUrl))
            continue
    print('第%d张下载成功' % imgnum)
```

在做了以上工作之外，还添加了`help`方法来引导用户使用，这个提示通过调用`py`文件是传入`-h`来调用:
```Python
def help():
    # 脚本使用帮助函数
    print("""
        本脚本用于下载单页面中的图片到指定位置
        -h:查看帮助文档
        -p:保存的路径（可选）
        -u:下载的页面URL（必填）
    """)
```

### [源代码](https://github.com/coldJune/Python/tree/master/catchZHwallpaper)
```Python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

import urllib.request as req
import urllib.parse as parse
import os
import uuid
from bs4 import BeautifulSoup
import urllib.error
import sys, getopt
import re


def craw(url, save_path):
    # 从指定单页面下载所有图片
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) '
                      'AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/48.0.2564.116 '
                      'Safari/537.36 '
                      'TheWorld 7'}
    request = req.Request(url=url, headers=headers)
    html = req.urlopen(request).read();
    html = str(html)
    soup = BeautifulSoup(html, 'lxml')
    noscripts = soup.findAll('noscript')
    imgurls = []
    for noscript in noscripts:
        # 提取图片的URL链接
        img = noscript.find('img')
        url = img['data-original'] if img['data-original'] else ''
        if url != '':
            imgurls.append(url)

    if save_path[0] != '/':
        save_path = './' + save_path
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    if save_path[-1] != '/':
        save_path += '/'

    print('该页面一共有%d张图片' % len(imgurls))
    imgnum = 0
    for imgUrl in imgurls:
        imgnum = imgnum + 1
        picname = save_path + str(uuid.uuid1())+'.jpg'
        print('正在下载%d张' % imgnum)
        try:
            req.urlretrieve(imgUrl, picname)
        except urllib.error.URLError as e:
            if hasattr(e, 'code') or hasattr(e, 'reason'):
                print('第%d张下载失败，其链接地址为%s' % (imgnum, imgUrl))
                continue
        print('第%d张下载成功' % imgnum)


def help():
    # 脚本使用帮助函数
    print("""
        本脚本用于下载单页面中的图片到指定位置
        -h:查看帮助文档
        -p:保存的路径（可选）
        -u:下载的页面URL（必填）
    """)


if __name__ == '__main__':
    opts, args = getopt.getopt(sys.argv[1:], 'hp:u:')
    furl, savepath = '', ''
    for op, value in opts:
        if op == '-u':
            furl = value
        elif op == '-p':
            savepath = value
        elif op == '-h':
            help()
            sys.exit()

    if furl == '':
        print('请输入下载链接，脚本使用方法通过-h查看')
    else:
        craw(furl, savepath)
```
