---
title: 金融和经济数据应用
date: 2018-03-29 08:38:08
categories: 数据分析
copyright: true
tags:
    - 应用示例
description: Python在金融行业中的应用越来越多，它不仅适合交互式的分析环境，也很适合开发稳健的系统，它还是很好的粘合层，可以非常轻松地位C或C++编写的库构建Python接口。
---

## 数据规整化
### 时间序列及截面对齐
对两个相关的时间序列的索引没有对齐或两个DataFrame对象可能含有不匹配的行或列，pandas在算数运算中自动对齐及解决合并时可能带来的bug；下面对[股票价格](https://github.com/coldJune/Python/blob/master/ipython/stock_px.csv)和[成交量](https://github.com/coldJune/Python/blob/master/ipython/volume.csv)进行计算:
```Python
In [14]: prices = pd.read_csv('stock_px.csv', parse_dates=True, index_col=0) #加载股票价格

In [15]: volume = pd.read_csv('volume.csv', parse_dates=True, index_col=0) #加载成交量

In [16]: pr = prices[:5]

In [17]: vo = volume[:3]

In [18]: pr
Out[18]:
              AA  AAPL    GE    IBM   JNJ  MSFT   PEP     SPX   XOM
1990-02-01  4.98  7.86  2.87  16.79  4.27  0.51  6.04  328.79  6.12
1990-02-02  5.04  8.00  2.87  16.89  4.37  0.51  6.09  330.92  6.24
1990-02-05  5.07  8.18  2.87  17.32  4.34  0.51  6.05  331.85  6.25
1990-02-06  5.01  8.12  2.88  17.56  4.32  0.51  6.15  329.66  6.23
1990-02-07  5.04  7.77  2.91  17.93  4.38  0.51  6.17  333.75  6.33

In [19]: vo
Out[19]:
                   AA       AAPL          GE        IBM        JNJ  \
1990-02-01  2185600.0  4193200.0  14457600.0  6903600.0  5942400.0
1990-02-02  3103200.0  4248800.0  15302400.0  6064400.0  4732800.0
1990-02-05  1792800.0  3653200.0   9134400.0  5299200.0  3950400.0

                  MSFT        PEP          SPX        XOM
1990-02-01  89193600.0  2954400.0  154580000.0  2916400.0
1990-02-02  71395200.0  2424000.0  164400000.0  4250000.0
1990-02-05  59731200.0  2225400.0  130950000.0  5880800.0

In [20]: #在加权成交量的加权平均价格时pandas会自动对齐数据，并在sum这样的函数中排除缺失数据

In [21]: pr * vo
Out[21]:
                    AA        AAPL          GE          IBM         JNJ  \
1990-02-01  10884288.0  32958552.0  41493312.0  115911444.0  25374048.0
1990-02-02  15640128.0  33990400.0  43917888.0  102427716.0  20682336.0
1990-02-05   9089496.0  29883176.0  26215728.0   91782144.0  17144736.0
1990-02-06         NaN         NaN         NaN          NaN         NaN
1990-02-07         NaN         NaN         NaN          NaN         NaN

                  MSFT         PEP           SPX         XOM
1990-02-01  45488736.0  17844576.0  5.082436e+10  17848368.0
1990-02-02  36411552.0  14762160.0  5.440325e+10  26520000.0
1990-02-05  30462912.0  13463670.0  4.345576e+10  36755000.0
1990-02-06         NaN         NaN           NaN         NaN
1990-02-07         NaN         NaN           NaN         NaN

In [22]: vwap = (pr*vo).sum()/vo.sum()

In [23]: vwap
Out[23]:
AA        5.029077
AAPL      8.005831
GE        2.870000
IBM      16.976948
JNJ       4.321267
MSFT      0.510000
PEP       6.058866
SPX     330.458880
XOM       6.217684
dtype: float64
```
使用DataFrame的`align`方法手工对齐，它返回一个元组，含有两个对象的重索引版本:
```Python
In [24]: pr.align(vo, join='inner')
Out[24]:
(              AA  AAPL    GE    IBM   JNJ  MSFT   PEP     SPX   XOM
 1990-02-01  4.98  7.86  2.87  16.79  4.27  0.51  6.04  328.79  6.12
 1990-02-02  5.04  8.00  2.87  16.89  4.37  0.51  6.09  330.92  6.24
 1990-02-05  5.07  8.18  2.87  17.32  4.34  0.51  6.05  331.85  6.25,
                    AA       AAPL          GE        IBM        JNJ  \
 1990-02-01  2185600.0  4193200.0  14457600.0  6903600.0  5942400.0
 1990-02-02  3103200.0  4248800.0  15302400.0  6064400.0  4732800.0
 1990-02-05  1792800.0  3653200.0   9134400.0  5299200.0  3950400.0

                   MSFT        PEP          SPX        XOM
 1990-02-01  89193600.0  2954400.0  154580000.0  2916400.0
 1990-02-02  71395200.0  2424000.0  164400000.0  4250000.0
 1990-02-05  59731200.0  2225400.0  130950000.0  5880800.0  )
```
通过一组索引可能不同的Series构建一个DataFrame，可以显式定义结果的索引(丢弃其余的数据):
```Python
In [25]: s1 = Series(range(3), index=list('abc'))

In [26]: s2 = Series(range(4), index=list('dbce'))

In [27]: s3 = Series(range(3),index=list('fac'))

In [28]: DataFrame({'one':s1,'two':s2,'three':s3})
Out[28]:
   one  three  two
a  0.0    1.0  NaN
b  1.0    NaN  1.0
c  2.0    2.0  2.0
d  NaN    NaN  0.0
e  NaN    NaN  3.0
f  NaN    0.0  NaN

In [29]: DataFrame({'one':s1,'two':s2,'three':s3},index=list('fabc'))#显示定义结果索引
Out[29]:
   one  three  two
f  NaN    0.0  NaN
a  0.0    1.0  NaN
b  1.0    NaN  1.0
c  2.0    2.0  2.0

```
### 频率不同的时间序列的运算
经济学时间序列常常有着按年、季、月、日计算的或其他更特殊的频率。频率转换和重对齐的两大主要工具是`resample`和`reindex`方法。[`resample`](http://coldjune.com/2018/03/28/时间序列-二/#重采样及频率转换)用于将数据转换到固定频率，而[`reindex`](http://coldjune.com/2018/03/19/pandas%E5%85%A5%E9%97%A8-%E4%BA%8C/#%E9%87%8D%E6%96%B0%E7%B4%A2%E5%BC%95)则用于使数据符合一个新索引，他们都支持插值逻辑：
```Python
In [30]: # 创建一个周型时间序列

In [31]: ts1 = Series(np.random.randn(3),index=pd.date_range('2018-3-29',periods=3,freq='W-WED'))

In [32]: ts1
Out[32]:
2018-04-04    1.512158
2018-04-11   -0.608786
2018-04-18   -0.851491
Freq: W-WED, dtype: float64

In [33]: # 将其重采样到工作日(一到五)频率，没有数据的日子就会是空

In [34]: ts1.resample('B').mean()
Out[34]:
2018-04-04    1.512158
2018-04-05         NaN
2018-04-06         NaN
2018-04-09         NaN
2018-04-10         NaN
2018-04-11   -0.608786
2018-04-12         NaN
2018-04-13         NaN
2018-04-16         NaN
2018-04-17         NaN
2018-04-18   -0.851491
Freq: B, dtype: float64

In [35]: #调用插值方法填充空白

In [36]: ts1.resample('B').mean().ffill()
Out[36]:
2018-04-04    1.512158
2018-04-05    1.512158
2018-04-06    1.512158
2018-04-09    1.512158
2018-04-10    1.512158
2018-04-11   -0.608786
2018-04-12   -0.608786
2018-04-13   -0.608786
2018-04-16   -0.608786
2018-04-17   -0.608786
2018-04-18   -0.851491
Freq: B, dtype: float64

In [44]: # 创建不规则样本的时间序列

In [45]: dates = pd.DatetimeIndex(['2018-3-29','2018-4-18','2018-4-11','2017-5-9'])

In [46]: ts2 = Series(np.random.randn(len(dates)),index=dates)

In [47]: #将ts1前向填充到ts2中保存提示索引

In [48]: ts1.reindex(ts2.index, method='ffill')
Out[48]:
2018-03-29         NaN
2018-04-18   -0.851491
2018-04-11   -0.608786
2017-05-09         NaN
dtype: float64

In [49]: ts1.reindex(ts2.index, method='ffill') +ts2
Out[49]:
2018-03-29         NaN
2018-04-18   -0.771631
2018-04-11    0.738167
2017-05-09         NaN
dtype: float64
```
#### 使用period
Period(表示时间区间)提供了处理那些有着特殊规范的以年或季度为频率的金融或经济序列，跟Timestamp的时间序列不同，由Period索引的不同频率的时间序列之间的运算必须进行显示转化:
```Python
In [53]: #创建GDP和通货膨胀的宏观经济时间序列

In [54]: gdp = Series([1.78,1.94,2.08,2.01,2.15,2.31,2.46],index=pd.period_range('1984Q2',periods=7,freq='Q-SEP'))

In [55]: infl = Series([0.025,0.045,0.037,0.04],index=pd.period_range('1982',periods=4, freq='A-DEC'))

In [57]: gdp
Out[57]:
1984Q2    1.78
1984Q3    1.94
1984Q4    2.08
1985Q1    2.01
1985Q2    2.15
1985Q3    2.31
1985Q4    2.46
Freq: Q-SEP, dtype: float64

In [58]: infl
Out[58]:
1982    0.025
1983    0.045
1984    0.037
1985    0.040
Freq: A-DEC, dtype: float64

In [59]: # 假设infl值是年末观测，可以将其转换到Q-SEP

In [60]: infl_q = infl.asfreq('Q-SEP', how='end')

In [61]: infl_q
Out[61]:
1983Q1    0.025
1984Q1    0.045
1985Q1    0.037
1986Q1    0.040
Freq: Q-SEP, dtype: float64

In [62]: #重索引使用前向填充

In [63]: infl_q.reindex(gdp.index, method='ffill')
Out[63]:
1984Q2    0.045
1984Q3    0.045
1984Q4    0.045
1985Q1    0.037
1985Q2    0.037
1985Q3    0.037
1985Q4    0.037
Freq: Q-SEP, dtype: float64
```

### 时间和"最当前"数据选取
处理观测值没有精确地落在期望的时间点上:
```Python
In [64]: #生成一个交易日内的日期范围

In [65]: rng = pd.date_range('3/29/2018 9:30','3/29/2018 15:59', freq='T')

In [66]: #生成5天的时间点(9:30~15:59之间的值)

In [67]: rng = rng.append([rng + pd.offsets.BDay(i) for i in range(1,4)])

In [68]: ts = Series(np.random.randn(len(rng)), index=rng)

In [69]: ts
Out[69]:
2018-03-29 09:30:00   -1.743902
2018-03-29 09:31:00    0.739490
2018-03-29 09:32:00   -0.760746
                         ...
2018-04-03 15:57:00   -0.924520
2018-04-03 15:58:00   -1.466870
2018-04-03 15:59:00    1.543675
Length: 1560, dtype: float64

In [70]: # 利用Python的datetime.time对象进行索引抽取时间点上的值

In [71]: from datetime import time

In [72]: ts[time(10,0)]
Out[72]:
2018-03-29 10:00:00   -0.063743
2018-03-30 10:00:00    0.573829
2018-04-02 10:00:00    1.351708
2018-04-03 10:00:00   -1.140183
dtype: float64

In [73]: # 等同于at_time

In [74]: ts.at_time(time(10,0))
Out[74]:
2018-03-29 10:00:00   -0.063743
2018-03-30 10:00:00    0.573829
2018-04-02 10:00:00    1.351708
2018-04-03 10:00:00   -1.140183
dtype: float64

In [75]: # between_time用于选取两个Time对象之间的值

In [76]: ts.between_time(time(10,0),time(10,1))
Out[76]:
2018-03-29 10:00:00   -0.063743
2018-03-29 10:01:00   -0.507813
2018-03-30 10:00:00    0.573829
2018-03-30 10:01:00   -0.622986
2018-04-02 10:00:00    1.351708
2018-04-02 10:01:00   -2.014871
2018-04-03 10:00:00   -1.140183
2018-04-03 10:01:00   -1.077669
dtype: float64

In [77]: # 将时间序列的大部分内容随机设置NA

In [78]: indexer = np.sort(np.random.permutation(len(ts))[700:])

In [80]: irr_ts = ts.copy()

In [81]: irr_ts[indexer] = np.nan

In [82]: irr_ts['2018-3-29 9:50':'2018-3-29 10:00']
Out[82]:
2018-03-29 09:50:00         NaN
2018-03-29 09:51:00    0.995501
2018-03-29 09:52:00         NaN
2018-03-29 09:53:00    0.940157
2018-03-29 09:54:00    0.189236
2018-03-29 09:55:00    1.575382
2018-03-29 09:56:00   -0.017763
2018-03-29 09:57:00   -1.478983
2018-03-29 09:58:00    1.099623
2018-03-29 09:59:00         NaN
2018-03-29 10:00:00   -0.063743
dtype: float64

In [83]: # 通过asof方法得到时间点处(或与之前最近)的有效值(非NA)

In [84]: selection = pd.date_range('2018-3-29 10:00',periods=4, freq='B')

In [85]: irr_ts.asof(selection)
Out[85]:
2018-03-29 10:00:00   -0.063743
2018-03-30 10:00:00    0.573829
2018-04-02 10:00:00    1.351708
2018-04-03 10:00:00   -1.190552
Freq: B, dtype: float64

```
### 拼接多个数据源
在金融或经济领域中，可能出现：
1. 在一个特定的时间点上，从一个数据源切换到另一个数据源
2. 用一个时间序列对当前时间序列中的缺失值打补丁
3. 将数据中的符号(国家，资产代码等)替换为实际数据
```Python
In [86]: data1 = DataFrame(np.ones((6,3),dtype=float),
    ...:                   columns=list('abc'),
    ...:                   index = pd.date_range('3/28/2018', periods=6))
    ...:

In [87]: data2 = DataFrame(np.ones((6,3),dtype=float)*2,
    ...:                   columns=list('abc'),
    ...:                   index = pd.date_range('3/31/2018', periods=6))
    ...:
    ...:

In [88]: #使用concat将两个TimeSeries或DataFrame对象合并到一起实现在特定时刻从一个时间序列切换到另一个

In [89]: spliced = pd.concat([data1.loc[:'2018-3-31'],data2.loc['2018-3-31':]])

In [90]: spliced
Out[90]:
              a    b    c
2018-03-28  1.0  1.0  1.0
2018-03-29  1.0  1.0  1.0
2018-03-30  1.0  1.0  1.0
2018-03-31  1.0  1.0  1.0
2018-03-31  2.0  2.0  2.0
2018-04-01  2.0  2.0  2.0
2018-04-02  2.0  2.0  2.0
2018-04-03  2.0  2.0  2.0
2018-04-04  2.0  2.0  2.0
2018-04-05  2.0  2.0  2.0

In [91]: #data1缺失了data2存在的某列时间序列

In [92]: data2 = DataFrame(np.ones((6,4),dtype=float)*2,
    ...:                   columns=list('abcd'),
    ...:                   index = pd.date_range('3/31/2018', periods=6))
    ...:

In [93]: spliced = pd.concat([data1.loc[:'2018-3-31'],data2.loc['2018-3-31':]])

In [94]: spliced
Out[94]:
              a    b    c    d
2018-03-28  1.0  1.0  1.0  NaN
2018-03-29  1.0  1.0  1.0  NaN
2018-03-30  1.0  1.0  1.0  NaN
2018-03-31  1.0  1.0  1.0  NaN
2018-03-31  2.0  2.0  2.0  2.0
2018-04-01  2.0  2.0  2.0  2.0
2018-04-02  2.0  2.0  2.0  2.0
2018-04-03  2.0  2.0  2.0  2.0
2018-04-04  2.0  2.0  2.0  2.0
2018-04-05  2.0  2.0  2.0  2.0

In [95]: #combine_first可以引入合并点之前的数据

In [96]: spliced_filled = spliced.combine_first(data2)

In [97]: spliced_filled
Out[97]:
              a    b    c    d
2018-03-28  1.0  1.0  1.0  NaN
2018-03-29  1.0  1.0  1.0  NaN
2018-03-30  1.0  1.0  1.0  NaN
2018-03-31  1.0  1.0  1.0  2.0
2018-03-31  2.0  2.0  2.0  2.0
2018-04-01  2.0  2.0  2.0  2.0
2018-04-02  2.0  2.0  2.0  2.0
2018-04-03  2.0  2.0  2.0  2.0
2018-04-04  2.0  2.0  2.0  2.0
2018-04-05  2.0  2.0  2.0  2.0

In [98]: #update方法可以实现就地更新，只想填补空值，必须传入overwrite=False

In [99]: spliced.update(data2, overwrite=False)

In [100]: spliced
Out[100]:
              a    b    c    d
2018-03-28  1.0  1.0  1.0  NaN
2018-03-29  1.0  1.0  1.0  NaN
2018-03-30  1.0  1.0  1.0  NaN
2018-03-31  1.0  1.0  1.0  2.0
2018-03-31  2.0  2.0  2.0  2.0
2018-04-01  2.0  2.0  2.0  2.0
2018-04-02  2.0  2.0  2.0  2.0
2018-04-03  2.0  2.0  2.0  2.0
2018-04-04  2.0  2.0  2.0  2.0
2018-04-05  2.0  2.0  2.0  2.0

In [101]: #索引机制实现数据中的符号替换为实际数据

In [102]: cp_spliced = spliced.copy()

In [104]: cp_spliced[['a','c']] = data1[['a','c']]

In [105]: cp_spliced
Out[105]:
              a    b    c    d
2018-03-28  1.0  1.0  1.0  NaN
2018-03-29  1.0  1.0  1.0  NaN
2018-03-30  1.0  1.0  1.0  NaN
2018-03-31  1.0  1.0  1.0  2.0
2018-03-31  1.0  2.0  1.0  2.0
2018-04-01  1.0  2.0  1.0  2.0
2018-04-02  1.0  2.0  1.0  2.0
2018-04-03  NaN  2.0  NaN  2.0
2018-04-04  NaN  2.0  NaN  2.0
2018-04-05  NaN  2.0  NaN  2.0
```

## 分组变换和分析
```Python

In [126]: # 随机生成1000个股票代码

In [127]: import random;random.seed(0)

In [128]: import string

In [129]: def rands(n):
     ...:     choices = string.ascii_uppercase
     ...:     return ''.join([random.choice(choices) for _ in range(n)])
     ...:

In [130]: tickers = np.array([rands(5) for _ in range(N)])

In [131]: # 创建一个含有3列的DataFrame来承载数据，只选取部分股票

In [132]: M = 500

In [133]: df = DataFrame({'Momentum':np.random.randn(M)/200 +0.03,
     ...:                 'Value':np.random.randn(M)/200 +0.08,
     ...:                 'ShortInterest':np.random.randn(M)/200 -0.02},
     ...:                 index=tickers[:M])
     ...:

In [134]: # 为股票随机创建一个行业分类

In [135]: ind_names = np.array(['FINANCIAL','TECH'])

In [136]: sampler = np.random.randint(0, len(ind_names), N)

In [137]: industries = Series(ind_names[sampler], index=tickers, name='industry')

In [138]: # 根据行业分类进行分组并执行分组聚合和变换

In [139]: by_industry = df.groupby(industries)

In [140]: by_industry.mean()
Out[140]:
           Momentum  ShortInterest     Value
industry
FINANCIAL  0.029571      -0.019912  0.079195
TECH       0.030190      -0.020191  0.080153

In [141]: by_industry.describe()
Out[141]:
          Momentum                                                    \
             count      mean       std       min       25%       50%
industry
FINANCIAL    250.0  0.029571  0.005004  0.017440  0.026313  0.029229
TECH         250.0  0.030190  0.005024  0.016312  0.026794  0.030389

                              ShortInterest              ...               \
                75%       max         count      mean    ...          75%
industry                                                 ...
FINANCIAL  0.033130  0.043352         250.0 -0.019912    ...    -0.016613
TECH       0.033411  0.049917         250.0 -0.020191    ...    -0.016571

                     Value                                                    \
                max  count      mean       std       min       25%       50%
industry
FINANCIAL -0.008360  250.0  0.079195  0.004889  0.063352  0.075909  0.079352
TECH      -0.006412  250.0  0.080153  0.005009  0.063693  0.076513  0.079783


                75%       max
industry
FINANCIAL  0.082551  0.095753
TECH       0.083312  0.093676

[2 rows x 24 columns]

In [142]: def zscore(group):
     ...:     #行业标准化处理
     ...:     return (group-group.mean())/group.std()
     ...:

In [143]: df_stand = by_industry.apply(zscore)

In [144]: # 处理之后各行业平均值为0，标准差为1

In [145]: df_stand.groupby(industries).agg(['mean','std'])
Out[145]:
               Momentum      ShortInterest              Value
                   mean  std          mean  std          mean  std
industry
FINANCIAL -1.207923e-16  1.0 -6.024070e-16  1.0 -5.688783e-15  1.0
TECH      -2.044143e-15  1.0 -1.899814e-15  1.0  6.292744e-15  1.0

In [146]: #使用内置变换函数

In [147]: ind_rank = by_industry.rank(ascending=False)

In [148]: ind_rank.groupby(industries).agg(['min','max'])
Out[148]:
          Momentum        ShortInterest        Value
               min    max           min    max   min    max
industry
FINANCIAL      1.0  250.0           1.0  250.0   1.0  250.0
TECH           1.0  250.0           1.0  250.0   1.0  250.0

In [151]: #通过rank和zscore函数实现排名和标准化

In [152]: by_industry.apply(lambda x: zscore(x.rank())) #行内排名和标准化
Out[152]:
       Momentum  ShortInterest     Value
MYNBI  1.334477       1.375963 -0.117545
...         ...            ...       ...
GXKFD  0.905785      -0.145202  0.919614

[500 rows x 3 columns]

```
### 分组因子暴露
**因子分析(factor analysis)** 是投资组合定量管理中的一种技术。投资组合的持有量和性能(收益和损失)可以被分解为一个或多个表示投资组合权重的因子(风险因子是其中之一)：
```Python
In [153]: #随机生成因子

In [154]: fac1,fac2,fac3 = np.random.rand(3,1000)

In [155]: ticker_subset = tickers.take(np.random.permutation(N)[:1000])

In [156]: # 因子加权和以及噪声

In [157]: port = Series(0.7*fac1-1.2*fac2+0.3*fac3+rand(1000),index=ticker_subset)

In [158]: factors = DataFrame({'f1':fac1,'f2':fac2,'f3':fac3},index=ticker_subset)

In [159]: #相关性

In [160]: factors.corrwith(port)
Out[160]:
f1    0.418068
f2   -0.686129
f3    0.170196
dtype: float64

In [161]: #使用最小二乘回归计算整个投资组合的暴露
In [193]: import statsmodels.api as sm

In [194]: sm.OLS(port,factors).fit().summary()
Out[194]:
<class 'statsmodels.iolib.summary.Summary'>
"""
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.747
Model:                            OLS   Adj. R-squared:                  0.746
Method:                 Least Squares   F-statistic:                     979.6
Date:                Thu, 29 Mar 2018   Prob (F-statistic):          1.15e-296
Time:                        14:39:55   Log-Likelihood:                -299.34
No. Observations:                1000   AIC:                             604.7
Df Residuals:                     997   BIC:                             619.4
Df Model:                           3
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
f1             1.0206      0.029     34.643      0.000       0.963       1.078
f2            -0.9222      0.030    -31.056      0.000      -0.981      -0.864
f3             0.6187      0.030     20.752      0.000       0.560       0.677
==============================================================================
Omnibus:                       82.903   Durbin-Watson:                   1.811
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               27.499
Skew:                           0.062   Prob(JB):                     1.07e-06
Kurtosis:                       2.197   Cond. No.                         3.16
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
"""
```
*****
**注：书本中大量源数据已经无法获取，所以只记录了由自己创造的那一步，而且涉及到大量金融经济相关术语，所以这一节只是作为练习熟悉部分方法**
